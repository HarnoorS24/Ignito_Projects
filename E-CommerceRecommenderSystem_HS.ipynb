{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b15d44b3",
   "metadata": {},
   "source": [
    "# E-Commerce Recommender System\n",
    "\n",
    "## Main Goals\n",
    "\n",
    "- Build a recommender system based on user purchase history.\n",
    "- Create user-based features through data aggregation.\n",
    "- Extract time-based features like customer recency and frequency.\n",
    "- Apply feature scaling to prepare user data for clustering.\n",
    "- Segment customers into distinct groups using a clustering algorithm.\n",
    "\n",
    "### Context\n",
    "\n",
    "In the vast and competitive landscape of e-commerce, providing personalized recommendations is essential for enhancing user experience, driving sales, and fostering customer loyalty. The primary challenge is to sift through massive product catalogs and complex user histories to suggest items that are genuinely relevant to an individual's tastes and needs. In the field of data science, recommender systems provide a powerful solution by analyzing past behavior to predict future preferences. This project uses a real-world dataset of e-commerce transactions to build a recommender system by creating detailed user profiles and segmenting customers into like-minded groups, enabling a more data-driven approach to product discovery.\n",
    "\n",
    "## 1. Loading in the Data\n",
    "\n",
    "For this project, we will use the [Online Retail Dataset](https://archive.ics.uci.edu/dataset/352/online+retail) from the UCI Machine Learning Repository. Please visit the UCI website to download the data file for this activity, and then upload the file to the same directory as the notebook file.\n",
    "\n",
    "Note that this is currently an excel file as opposed to a csv. Opening it in excel, and then clicking file, gives you the option to save this as a csv, so it is recommended to do that. pandas may have an issue reading the file otherwise.\n",
    "\n",
    "We can start by loading in the dataset into a pandas dataframe, and then displaying it to ensure it loaded correctly, and so we can see what the features like `CustomerID`, `InvoiceNo`, and `Quantity` look like. This means that we have to start by importing pandas as well.\n",
    "\n",
    "It's worth mentioning that anytime you have a dataset from an external source, in this case the UCI repository, you can and should refer back to the source of the data to clear up misconceptions and also to get a better understanding of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce85df39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 541909 entries, 0 to 541908\n",
      "Data columns (total 8 columns):\n",
      " #   Column       Non-Null Count   Dtype  \n",
      "---  ------       --------------   -----  \n",
      " 0   InvoiceNo    541909 non-null  object \n",
      " 1   StockCode    541909 non-null  object \n",
      " 2   Description  540455 non-null  object \n",
      " 3   Quantity     541909 non-null  int64  \n",
      " 4   InvoiceDate  541909 non-null  object \n",
      " 5   UnitPrice    541909 non-null  float64\n",
      " 6   CustomerID   406829 non-null  float64\n",
      " 7   Country      541909 non-null  object \n",
      "dtypes: float64(2), int64(1), object(5)\n",
      "memory usage: 33.1+ MB\n",
      "None\n",
      "            Quantity      UnitPrice     CustomerID\n",
      "count  541909.000000  541909.000000  406829.000000\n",
      "mean        9.552250       4.611114   15287.690570\n",
      "std       218.081158      96.759853    1713.600303\n",
      "min    -80995.000000  -11062.060000   12346.000000\n",
      "25%         1.000000       1.250000   13953.000000\n",
      "50%         3.000000       2.080000   15152.000000\n",
      "75%        10.000000       4.130000   16791.000000\n",
      "max     80995.000000   38970.000000   18287.000000\n"
     ]
    }
   ],
   "source": [
    "#import pandas\n",
    "import pandas as pd\n",
    "\n",
    "#load the dataset\n",
    "df = pd.read_csv('Online Retail.csv')\n",
    "\n",
    "#display info of the data\n",
    "print(df.info())\n",
    "print(df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "738a1d95",
   "metadata": {},
   "source": [
    "Having taken a look at our data, there is a lot to take note of. Let's clarify the features below using information from UCI archive.\n",
    "\n",
    "- `InvoiceNo`: A 6-digit integral number uniquely assigned to each transaction. If this code starts with letter 'c', it indicates a cancellation. Since it can include the 'c', it's likely a string, and is categorical.\n",
    "- `StockCode`: A 5-digit integral number uniquely assigned to each distinct product. Some do have letters, making this a categorical feature as well.\n",
    "- `Description`: The name of the product purchased.\n",
    "- `Quantity`: An integer representing the amount of each product (item) per transaction.\n",
    "- `InvoiceDate`: Day and time when each transaction was generated. We'll need to check if it's in datetime format.\n",
    "- `UnitPrice`: Product price per unit. In this case, it's per sterling, as this dataset is based on infromation from the UK.\n",
    "- `CustomerID`: A 5-digit integral number uniquely assigned to each customer. \n",
    "- `Country`: Name of the country where each customer resides. Given this is from an online store, people could purchase from this store from anywhere.\n",
    "\n",
    "With a better understanding of our data, let's move on to preprocessing.\n",
    "\n",
    "## 2. Preprocessing\n",
    "Having taken a good look at the data, we can now start to clean it. Before we split the data or build the model, it is important to make sure the data is ready for the model and any other transformations.\n",
    "\n",
    "### Handling Null Entries\n",
    "While the UCI archive says that there are no missing entries, inspecting our dataframe actually suggests otherwise, as both CustomerID and Description seems to be missing values. Let's confirm this by explicitly checking for null entries, and then remove rows as necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8e507b66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "InvoiceNo           0\n",
      "StockCode           0\n",
      "Description      1454\n",
      "Quantity            0\n",
      "InvoiceDate         0\n",
      "UnitPrice           0\n",
      "CustomerID     135080\n",
      "Country             0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#check for null values\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "780c4074",
   "metadata": {},
   "source": [
    "Just as we expected unfortunately, we are missing a lot of the CustomerIDs. Although we will be losing a lot of data by having to remove the rows where CustomerID is null, we can't make any predictions or references if we don't do so. As such, let's go ahead and remove these rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "03cb5a96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "InvoiceNo      0\n",
      "StockCode      0\n",
      "Description    0\n",
      "Quantity       0\n",
      "InvoiceDate    0\n",
      "UnitPrice      0\n",
      "CustomerID     0\n",
      "Country        0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#Remove rows where CustomerID is null\n",
    "df = df.dropna(subset=['CustomerID'])\n",
    "\n",
    "#Check again for null values\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c43f48da",
   "metadata": {},
   "source": [
    "With that, we have dealt with the rows that can't be used for machine learning. Fortunately, all the rows with a missing description were also dropped due to this. Since we would still be able to know who bought what even without the description, there wouldn't be a point in dropping those rows, and we would have to have done extra work in order to impute the missing data. Since we don't have to worry about that, let's move on.\n",
    "\n",
    "### Handling Irrelevant Data\n",
    "Something else we have to worry about is data that won't help us or our model. For example, we'll want to filter out rows where the the price of an item is 0, or possibly even listed as less than 0. This might be because it was a promotional item or a test transaction. We'll also make sure that charges unrelated to actual purchases, like bank transactions and postage or shipping, is removed as well, as they don't tell us anything about which items the customer would purchase.\n",
    "\n",
    "We'll filter out the dataframe with this in mind."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b43ba9a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape after removing non-positive UnitPrice: (406789, 8)\n",
      "Shape after removing non-product StockCodes: (405022, 8)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>InvoiceNo</th>\n",
       "      <th>StockCode</th>\n",
       "      <th>Description</th>\n",
       "      <th>Quantity</th>\n",
       "      <th>InvoiceDate</th>\n",
       "      <th>UnitPrice</th>\n",
       "      <th>CustomerID</th>\n",
       "      <th>Country</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>536365</td>\n",
       "      <td>85123A</td>\n",
       "      <td>WHITE HANGING HEART T-LIGHT HOLDER</td>\n",
       "      <td>6</td>\n",
       "      <td>12/1/2010 8:26</td>\n",
       "      <td>2.55</td>\n",
       "      <td>17850.0</td>\n",
       "      <td>United Kingdom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>536365</td>\n",
       "      <td>71053</td>\n",
       "      <td>WHITE METAL LANTERN</td>\n",
       "      <td>6</td>\n",
       "      <td>12/1/2010 8:26</td>\n",
       "      <td>3.39</td>\n",
       "      <td>17850.0</td>\n",
       "      <td>United Kingdom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>536365</td>\n",
       "      <td>84406B</td>\n",
       "      <td>CREAM CUPID HEARTS COAT HANGER</td>\n",
       "      <td>8</td>\n",
       "      <td>12/1/2010 8:26</td>\n",
       "      <td>2.75</td>\n",
       "      <td>17850.0</td>\n",
       "      <td>United Kingdom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>536365</td>\n",
       "      <td>84029G</td>\n",
       "      <td>KNITTED UNION FLAG HOT WATER BOTTLE</td>\n",
       "      <td>6</td>\n",
       "      <td>12/1/2010 8:26</td>\n",
       "      <td>3.39</td>\n",
       "      <td>17850.0</td>\n",
       "      <td>United Kingdom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>536365</td>\n",
       "      <td>84029E</td>\n",
       "      <td>RED WOOLLY HOTTIE WHITE HEART.</td>\n",
       "      <td>6</td>\n",
       "      <td>12/1/2010 8:26</td>\n",
       "      <td>3.39</td>\n",
       "      <td>17850.0</td>\n",
       "      <td>United Kingdom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>541904</th>\n",
       "      <td>581587</td>\n",
       "      <td>22613</td>\n",
       "      <td>PACK OF 20 SPACEBOY NAPKINS</td>\n",
       "      <td>12</td>\n",
       "      <td>12/9/2011 12:50</td>\n",
       "      <td>0.85</td>\n",
       "      <td>12680.0</td>\n",
       "      <td>France</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>541905</th>\n",
       "      <td>581587</td>\n",
       "      <td>22899</td>\n",
       "      <td>CHILDREN'S APRON DOLLY GIRL</td>\n",
       "      <td>6</td>\n",
       "      <td>12/9/2011 12:50</td>\n",
       "      <td>2.10</td>\n",
       "      <td>12680.0</td>\n",
       "      <td>France</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>541906</th>\n",
       "      <td>581587</td>\n",
       "      <td>23254</td>\n",
       "      <td>CHILDRENS CUTLERY DOLLY GIRL</td>\n",
       "      <td>4</td>\n",
       "      <td>12/9/2011 12:50</td>\n",
       "      <td>4.15</td>\n",
       "      <td>12680.0</td>\n",
       "      <td>France</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>541907</th>\n",
       "      <td>581587</td>\n",
       "      <td>23255</td>\n",
       "      <td>CHILDRENS CUTLERY CIRCUS PARADE</td>\n",
       "      <td>4</td>\n",
       "      <td>12/9/2011 12:50</td>\n",
       "      <td>4.15</td>\n",
       "      <td>12680.0</td>\n",
       "      <td>France</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>541908</th>\n",
       "      <td>581587</td>\n",
       "      <td>22138</td>\n",
       "      <td>BAKING SET 9 PIECE RETROSPOT</td>\n",
       "      <td>3</td>\n",
       "      <td>12/9/2011 12:50</td>\n",
       "      <td>4.95</td>\n",
       "      <td>12680.0</td>\n",
       "      <td>France</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>405022 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       InvoiceNo StockCode                          Description  Quantity  \\\n",
       "0         536365    85123A   WHITE HANGING HEART T-LIGHT HOLDER         6   \n",
       "1         536365     71053                  WHITE METAL LANTERN         6   \n",
       "2         536365    84406B       CREAM CUPID HEARTS COAT HANGER         8   \n",
       "3         536365    84029G  KNITTED UNION FLAG HOT WATER BOTTLE         6   \n",
       "4         536365    84029E       RED WOOLLY HOTTIE WHITE HEART.         6   \n",
       "...          ...       ...                                  ...       ...   \n",
       "541904    581587     22613          PACK OF 20 SPACEBOY NAPKINS        12   \n",
       "541905    581587     22899         CHILDREN'S APRON DOLLY GIRL          6   \n",
       "541906    581587     23254        CHILDRENS CUTLERY DOLLY GIRL          4   \n",
       "541907    581587     23255      CHILDRENS CUTLERY CIRCUS PARADE         4   \n",
       "541908    581587     22138        BAKING SET 9 PIECE RETROSPOT          3   \n",
       "\n",
       "            InvoiceDate  UnitPrice  CustomerID         Country  \n",
       "0        12/1/2010 8:26       2.55     17850.0  United Kingdom  \n",
       "1        12/1/2010 8:26       3.39     17850.0  United Kingdom  \n",
       "2        12/1/2010 8:26       2.75     17850.0  United Kingdom  \n",
       "3        12/1/2010 8:26       3.39     17850.0  United Kingdom  \n",
       "4        12/1/2010 8:26       3.39     17850.0  United Kingdom  \n",
       "...                 ...        ...         ...             ...  \n",
       "541904  12/9/2011 12:50       0.85     12680.0          France  \n",
       "541905  12/9/2011 12:50       2.10     12680.0          France  \n",
       "541906  12/9/2011 12:50       4.15     12680.0          France  \n",
       "541907  12/9/2011 12:50       4.15     12680.0          France  \n",
       "541908  12/9/2011 12:50       4.95     12680.0          France  \n",
       "\n",
       "[405022 rows x 8 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Remove rows with zero or negative UnitPrice\n",
    "#These are not standard purchases and can skew analysis.\n",
    "df = df[df['UnitPrice'] > 0]\n",
    "\n",
    "print(f\"Shape after removing non-positive UnitPrice: {df.shape}\")\n",
    "\n",
    "#Remove rows with non-product StockCodes\n",
    "#Filter out common operational codes like 'POST', 'M', 'BANK CHARGES', etc.\n",
    "#We also filter out any StockCodes that are purely alphabetical, as these\n",
    "#are typically not products (e.g., 'D' for discount, 'S' for samples).\n",
    "df = df[~df['StockCode'].str.match('^[A-Z]+$', na=False)]\n",
    "\n",
    "print(f\"Shape after removing non-product StockCodes: {df.shape}\")\n",
    "\n",
    "#Display the cleaned data\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c967874d",
   "metadata": {},
   "source": [
    "### Correcting the Date\n",
    "We noticed earlier that our date is in an 'object' format. While this makes sense for us, it isn't exactly the best format for any machine learning model. As such, we'll go ahead and convert this feature into datetime format. By using the .to_datetime function, we can do just that. By having it in the special datetime format, we'll be able organize the data by recency and give weights to more recents.\n",
    "\n",
    "As such, we'll go ahead and convert the data into datetime format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "485fa753",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "object\n",
      "datetime64[ns]\n"
     ]
    }
   ],
   "source": [
    "#correcting\n",
    "#check the invoicedate data type \n",
    "print(df['InvoiceDate'].dtype)\n",
    "\n",
    "#convert the InvoiceDate to datetime format\n",
    "df['InvoiceDate'] = pd.to_datetime(df['InvoiceDate'])\n",
    "\n",
    "#Check the data type again\n",
    "print(df['InvoiceDate'].dtype)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1d5d1af",
   "metadata": {},
   "source": [
    "## 3. Feature Engineering\n",
    "\n",
    "With our data cleaned, we can now proceed to the most critical phase for this project: **feature engineering**. The objective here is to transform our long list of individual transactions into a structured summary of each customer's unique behavior. We will create a new DataFrame where each row represents a single `CustomerID`, and the columns are metrics that quantitatively describe their purchasing habits, such as how often they shop, how much they spend, and how recently they've been active. This user-centric view is what will allow us to intelligently group similar customers and provide relevant recommendations.\n",
    "\n",
    "To accomplish this, we will primarily use the powerful `groupby('CustomerID').agg()` method in pandas, which allows us to calculate multiple summary statistics for each user at once. The specific features we plan to engineer are:\n",
    "\n",
    "* **`recency_days`**: Days since their last purchase.\n",
    "* **`frequency`**: Total number of unique invoices per customer\n",
    "* **`total_price`**: Quantity of an item multiplied by the price.\n",
    "* **`total_spent`**: The sum of `TotalPrice` for all their purchases.\n",
    "* **`avg_order_value`**: The average spending per transaction.\n",
    "* **`unique_items`**: The count of unique `StockCode`s they have purchased.\n",
    "* **`avg_items_per_order`**: The average number of items (`Quantity`) per transaction.\n",
    "* **`avg_days_between_purchases`**: The average time gap between their transactions.\n",
    "\n",
    "So, let's go ahead and create these features. This might be a bit longer of a code segment than usual so please do your best to follow along."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "99536d72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Feature Engineering Complete ---\n",
      "Preview of the final user_features DataFrame:\n",
      "            recency_days  frequency  total_spent  unique_items  total_items  \\\n",
      "CustomerID                                                                    \n",
      "12346.0              326          2         0.00             1            0   \n",
      "12347.0                2          7      4310.00           103         2458   \n",
      "12348.0               75          4      1437.24            21         2332   \n",
      "12349.0               19          1      1457.55            72          630   \n",
      "12350.0              310          1       294.40            16          196   \n",
      "\n",
      "            avg_order_value  avg_items_per_order  avg_days_between  \n",
      "CustomerID                                                          \n",
      "12346.0            0.000000             0.000000          0.000000  \n",
      "12347.0          615.714286           351.142857          2.000000  \n",
      "12348.0          359.310000           583.000000         10.846154  \n",
      "12349.0         1457.550000           630.000000          0.000000  \n",
      "12350.0          294.400000           196.000000          0.000000  \n"
     ]
    }
   ],
   "source": [
    "#Import numpy for numerical operations\n",
    "import numpy as np\n",
    "\n",
    "#Create TotalPrice Column\n",
    "df['total_price'] = df['Quantity'] * df['UnitPrice']\n",
    "\n",
    "\n",
    "#Calculate Recency, Frequency, and Monetary (RFM) values\n",
    "\n",
    "#For Recency, we need a snapshot date to calculate days since last purchase.\n",
    "#We'll use the day after the last transaction in the dataset as our reference.\n",
    "#timedelta is used to add a day to the max date.\n",
    "snapshot_date = df['InvoiceDate'].max() + pd.Timedelta(days=1)\n",
    "\n",
    "# Group by customer and calculate aggregations\n",
    "user_features = df.groupby('CustomerID').agg(\n",
    "    #Recency: Days since last purchase\n",
    "    recency_days=('InvoiceDate', lambda date: (snapshot_date - date.max()).days),\n",
    "    \n",
    "    #Frequency: Count of unique invoices\n",
    "    frequency=('InvoiceNo', 'nunique'),\n",
    "    \n",
    "    #Monetary: Sum of total price for all purchases\n",
    "    total_spent=('total_price', 'sum'),\n",
    "\n",
    "    #Other behavioral features\n",
    "    unique_items=('StockCode', 'nunique'),\n",
    "    total_items=('Quantity', 'sum')\n",
    ")\n",
    "\n",
    "\n",
    "#Calculate Additional Features\n",
    "#Average Order Value\n",
    "user_features['avg_order_value'] = user_features['total_spent'] / user_features['frequency']\n",
    "\n",
    "#Average Items per Order\n",
    "user_features['avg_items_per_order'] = user_features['total_items'] / user_features['frequency']\n",
    "\n",
    "\n",
    "#Calculate Average Days Between Purchases\n",
    "#First, get the list of purchase dates for each customer\n",
    "customer_dates = df.groupby('CustomerID')['InvoiceDate'].apply(list)\n",
    "\n",
    "#Calculate the time difference between consecutive purchases\n",
    "def get_avg_purchase_gap(dates):\n",
    "    if len(dates) > 1:\n",
    "        dates = sorted(dates)\n",
    "        gaps = [(dates[i] - dates[i-1]).days for i in range(1, len(dates))]\n",
    "        return np.mean(gaps)\n",
    "    return 0 # If only one purchase, return 0. Alternatively, we could return a large number to tell that this was a one-time purchase.\n",
    "\n",
    "user_features['avg_days_between'] = customer_dates.apply(get_avg_purchase_gap)\n",
    "\n",
    "\n",
    "#Display the Final User Features DataFrame\n",
    "print(\"--- Feature Engineering Complete ---\")\n",
    "print(\"Preview of the final user_features DataFrame:\")\n",
    "print(user_features.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b3c4239",
   "metadata": {},
   "source": [
    "It was quite a bit of work, but we were able to create a new dataframe that helps us with our purposes. In this dataframe, the index is the customer, so now each row represents a specific customer. Using infromation from this dataframe and our original dataframe, we'll be able to create a list of items to recommend to each customer. But before getting to that point, there's still more tranformations and functions we want to apply.\n",
    "\n",
    "## 4. Scaling\n",
    "\n",
    "For this project, there isn't exactly a model to built. We'll be using different techniques to create an algorithm to recommend certain products to customers. This also means that there won't be a train-test split for this project, and that we can go ahead and start scaling our data. The data will be scaled in a way so that the mean of the features is 0 and the standard deviation of them is 1. \n",
    "\n",
    "We'll only be scaling our user features dataframe, as we plan to use the data from this dataframe as a direct input to the k-means clustering technique (which will be explained more after scaling). Additionally, our originally dataframe includes data that wouldn't benefit from being scaled, such as numerical codes or categorical information.\n",
    "\n",
    "As such we'll go ahead and scale our user features dataframe by importing the standard scaler, fitting, and then transforming the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c6e7bb19",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import StandardScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "#Initialize the StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "#Fit and transform the user features DataFrame\n",
    "user_features_scaled = scaler.fit_transform(user_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e0b25cb",
   "metadata": {},
   "source": [
    "## 5. Clustering the Data\n",
    "\n",
    "With our user features now engineered and scaled, the next phase is to perform **customer segmentation** using a clustering algorithm. The goal here is to move beyond analyzing customers individually and instead discover natural groupings of users who exhibit similar purchasing behaviors. By identifying these segments, we can understand our customer base on a deeper level and tailor our recommendation strategy to the unique preferences of each group. This approach allows us to provide more relevant suggestions than a one-size-fits-all model.\n",
    "\n",
    "To achieve this, we will use **K-Means**, a powerful and widely-used clustering algorithm. We will start by choosing the number of clusters (`k`) we want to partition our data into. We will then initialize the K-Means model with this `k` value and fit it to our scaled `user_features` DataFrame. The algorithm will then assign each customer to a specific cluster. Finally, we will analyze the characteristics of each cluster by examining the average feature values of its members, allowing us to create specific groups of customers with specific spending habits. We can work from there to recommend products."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67956fa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Cluster Analysis Results ---\n",
      "Average feature values for each customer segment:\n",
      "         recency_days   frequency    total_spent  unique_items    total_items  \\\n",
      "cluster                                                                         \n",
      "0           55.516667    3.477778     593.947889     11.961111     289.261111   \n",
      "1           51.318102    2.383128     978.436274     40.516696     651.579965   \n",
      "2           24.000000   24.000000  123638.180000    443.000000   76946.000000   \n",
      "3          205.000000    2.250000    3504.450000      9.000000    7196.000000   \n",
      "4          242.418136    1.612091     305.423023     18.904282     155.450882   \n",
      "5           29.266917    6.259398    1768.340075    112.569549    1086.509398   \n",
      "6            8.426471   28.779412    8711.416765    262.161765    4934.014706   \n",
      "7            1.500000  193.000000   35409.875000   1548.500000   23413.000000   \n",
      "8           47.000000    2.000000     501.565000      1.500000     793.000000   \n",
      "9           96.148148    2.759259    4224.572037     58.685185    3104.407407   \n",
      "10           3.500000   31.000000   52060.435000   1001.000000   37178.500000   \n",
      "11           4.500000   54.500000  224696.415000    137.000000   66582.500000   \n",
      "12           2.000000   73.000000  278778.020000    699.000000  196556.000000   \n",
      "13          96.555556    1.777778    8268.416667     76.444444    3598.666667   \n",
      "14          84.533333    3.300000     695.421333      3.833333     938.366667   \n",
      "15          19.000000   10.406015    3602.929474    282.887218    2219.744361   \n",
      "16          46.111111    2.444444    1043.861111      1.888889     377.000000   \n",
      "17          10.500000   34.083333   42226.850000    119.250000   28691.500000   \n",
      "18          18.033019   16.410377    4425.028349    105.466981    2494.221698   \n",
      "19          32.625000    3.219466     582.963865     31.781489     323.545802   \n",
      "20          40.483871    3.854839    3140.428495     99.516129    1935.462366   \n",
      "21         225.297521    1.644628    1089.116942     44.206612     650.677686   \n",
      "22           1.666667  120.666667   42795.773333    679.000000   24825.666667   \n",
      "23         144.126338    2.584582     522.813233     31.008565     284.593148   \n",
      "24         182.000000    4.000000   39619.500000      1.000000      61.000000   \n",
      "25           4.363636   44.272727   21220.481818    109.000000   11090.318182   \n",
      "26           5.666667   55.666667   88522.540000    515.666667   60616.000000   \n",
      "27         336.833333    1.434028     232.972361     17.711806     119.572917   \n",
      "28          87.000000    1.000000    4314.720000     16.000000    7824.000000   \n",
      "29           1.000000  243.000000  133007.130000   1792.000000   76931.000000   \n",
      "\n",
      "         avg_order_value  avg_items_per_order  avg_days_between  \n",
      "cluster                                                          \n",
      "0             167.735747            82.165955         18.498293  \n",
      "1             411.251876           275.197086          1.743672  \n",
      "2            5151.590833          3206.083333          0.390686  \n",
      "3            2064.450000          3596.000000          2.380000  \n",
      "4             200.543946           102.402788          0.977902  \n",
      "5             291.584752           179.860027          1.747060  \n",
      "6             322.070420           185.889053          0.724745  \n",
      "7             189.574514           122.847790          0.051605  \n",
      "8             250.782500           396.500000        258.750000  \n",
      "9            1476.612997          1111.783514          2.057782  \n",
      "10           2131.562634          1125.148366          0.114691  \n",
      "11           4099.887670          1238.866667          0.921952  \n",
      "12           3818.876986          2692.547945          0.159146  \n",
      "13           4601.913704          2117.148148          1.399346  \n",
      "14            188.868296           253.982302         53.149616  \n",
      "15            364.762288           227.157081          0.691294  \n",
      "16            372.912037           139.981481        120.185185  \n",
      "17           1505.490071          1013.792921          2.263869  \n",
      "18            274.919524           156.697645          1.958521  \n",
      "19            178.953051           101.738485          2.767520  \n",
      "20            812.476633           526.769694          1.706091  \n",
      "21            675.592803           405.514664          0.878218  \n",
      "22            360.598725           209.316250          0.139965  \n",
      "23            203.634692           115.082769          2.163236  \n",
      "24           9904.875000            15.250000          0.000000  \n",
      "25            562.729230           285.338872          1.517337  \n",
      "26           1608.059518          1120.661688          0.386796  \n",
      "27            176.070922            91.783470          0.397503  \n",
      "28           4314.720000          7824.000000          0.000000  \n",
      "29            547.354444           316.588477          0.047668  \n"
     ]
    }
   ],
   "source": [
    "#Import KMeans for clustering\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "#We'll start by choosing a number of clusters, k. For this example, let's set k to 30, but we set this as a variable so we can easily change it\n",
    "k = 30\n",
    "\n",
    "#Initialize the KMeans model with k clusters\n",
    "#We use 'k-means++' for better initialization of centroids, and set n_init to 10 for robustness.\n",
    "kmeans = KMeans(n_clusters=k, init='k-means++', n_init=10, random_state=42)\n",
    "kmeans.fit(user_features_scaled) \n",
    "\n",
    "#Get the cluster assignment for each customer\n",
    "cluster_labels = kmeans.labels_\n",
    "\n",
    "#We add the cluster labels back to the original, unscaled DataFrame\n",
    "#so we can interpret the results with human-readable values.\n",
    "user_features['cluster'] = cluster_labels\n",
    "\n",
    "#We can now analyze the characteristics of each cluster by grouping\n",
    "#by the new 'cluster' column and calculating the mean of each feature.\n",
    "cluster_analysis = user_features.groupby('cluster').mean()\n",
    "\n",
    "print(\"Average feature values for each customer segment:\")\n",
    "print(cluster_analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9814f9f9",
   "metadata": {},
   "source": [
    "From our results, we can see the characteristics of each cluster. How much money on average a person from a specific might have spent, the average number of items bought, and much more is shown to us. It's with this information that we'll go ahead and build our recommender system. To make things easier, we'll map the cluster labels to our original transactions so that we know which cluster each purchase belongs to. We'll do this by merging the cluster labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c3bcd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We select only the cluster column for the merge\n",
    "customer_clusters = user_features[['cluster']]\n",
    "\n",
    "#Merge the cluster information back into the main dataframe\n",
    "df_with_clusters = df.merge(customer_clusters, on='CustomerID')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47904bea",
   "metadata": {},
   "source": [
    "## 6. Creating the Recommendation Logic\n",
    "With our customer segments defined by the clustering algorithm, we will now build the core recommendation logic. The goal is to provide personalized suggestions to a user by leveraging the collective wisdom of their peer group, that being the other customers within their assigned cluster. This approach is a form of collaborative filtering, where we assume that users with similar purchasing behaviors will have similar tastes in products.\n",
    "\n",
    "To accomplish this, we will create a function that, for any given `CustomerID`, first identifies which cluster they belong to. It then analyzes all purchases made by the other customers in that same cluster to determine which products are most popular among that specific segment. Finally, to ensure the recommendations are new and helpful, the function will filter out any items the original user has already bought and return a ranked list of the top remaining products, providing a targeted and relevant set of suggestions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb7e13be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommendations for Customer 17850:\n",
      "['85099B', '22423', '23203', '47566', '22720']\n"
     ]
    }
   ],
   "source": [
    "#Create a function for our recommendation logic\n",
    "#This function will take a customer ID and return a list of recommended items based on their cluster\n",
    "def get_recommendations(customer_id, top_n=5):\n",
    "    # 1. Get the target user's cluster\n",
    "    user_cluster = df_with_clusters.loc[df_with_clusters['CustomerID'] == customer_id, 'cluster'].iloc[0]\n",
    "\n",
    "    # 2. Get the list of items the user has already bought\n",
    "    user_purchases = df_with_clusters.loc[df_with_clusters['CustomerID'] == customer_id, 'StockCode'].unique()\n",
    "\n",
    "    # 3. Get purchase data for the user's peer group\n",
    "    peer_group = df_with_clusters[df_with_clusters['cluster'] == user_cluster]\n",
    "    \n",
    "    # 4. Find the most popular items in the peer group, excluding the user's own purchases\n",
    "    popular_items = peer_group[~peer_group['StockCode'].isin(user_purchases)]\n",
    "    recommendations = popular_items['StockCode'].value_counts().head(top_n).index.tolist()\n",
    "    \n",
    "    return recommendations\n",
    "\n",
    "#Test with a sample customer\n",
    "sample_customer = 17850 # Example customer ID. Just the first one in the dataset.\n",
    "recommendations = get_recommendations(sample_customer, top_n=3)\n",
    "\n",
    "print(f\"Recommendations for Customer {sample_customer}:\")\n",
    "print(recommendations)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c11e51bd",
   "metadata": {},
   "source": [
    "## 7. Refining the Recommender System\n",
    "While we were able to make our Recommender system, we currently don't have an idea as to what it's recommending to us, or how good these recommendations are. We can start by figuring out what is being recommended.\n",
    "\n",
    "We'll first start by creating a map for the stock codes. We want each unique stock code and it's description in a dictionary so that for any given code, we know what is being recommended. After running our function, we can save the recommendations and then translate them using the stock code map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e131b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommended StockCodes for Customer 17850:\n",
      "['85099B', '22423', '23203']\n",
      "\n",
      "Translated Recommendations:\n",
      "['JUMBO BAG RED RETROSPOT', 'REGENCY CAKESTAND 3 TIER', 'JUMBO BAG DOILEY PATTERNS']\n"
     ]
    }
   ],
   "source": [
    "#Create a mapping dictionary from StockCode to Description\n",
    "#We drop duplicates to ensure each StockCode maps to a single, clean Description\n",
    "stock_code_map = df.drop_duplicates(subset=['StockCode']).set_index('StockCode')['Description'].to_dict()\n",
    "\n",
    "#Get the recommended StockCodes\n",
    "sample_customer = 17850\n",
    "recommended_codes = get_recommendations(sample_customer, top_n=3)\n",
    "\n",
    "#Translate the codes into human-readable names using the map\n",
    "recommended_items = [stock_code_map[code] for code in recommended_codes]\n",
    "\n",
    "#Display the final, human-readable recommendations\n",
    "print(f\"Recommended StockCodes for Customer {sample_customer}:\")\n",
    "print(recommended_codes)\n",
    "print(\"\\nTranslated Recommendations:\")\n",
    "print(recommended_items)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f896ca87",
   "metadata": {},
   "source": [
    "Just like that, we know what is being recommended. Now this change could have simply been added to our recommendation logic function, but for the sake of listing each step one by one, we have it listed separate.\n",
    "\n",
    "## 8. Evaulating the System\n",
    "\n",
    "Finally, we must evaluate the quality of our recommender system to ensure it provides sensible suggestions. Since we are not predicting a single \"correct\" answer, traditional metrics like accuracy or RMSE do not apply here. Instead, we will perform a **qualitative analysis**, which involves a manual, logical inspection of the recommendations for a sample customer to determine if they are relevant and logical.\n",
    "\n",
    "To do this, we will create a script that profiles a specific customer, analyzes the characteristics of the peer group they belong to, and then presents the final recommendations. The process involves selecting a sample `CustomerID`, retrieving their engineered features and assigned cluster, and then comparing their individual profile to the average profile of their cluster. By examining the recommended items in the context of both the individual's and the group's purchasing behavior, we can make a well-informed judgment about whether the recommendations are logical and add value, thereby validating the effectiveness of our segmentation strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b6aafe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Analysis for Customer ID: 17850 ---\n",
      "\n",
      "## Step 1: Customer's Personal Profile\n",
      "recency_days            302.000000\n",
      "frequency                35.000000\n",
      "total_spent            5288.630000\n",
      "unique_items             24.000000\n",
      "total_items            1693.000000\n",
      "avg_order_value         151.103714\n",
      "avg_items_per_order      48.371429\n",
      "avg_days_between          0.221865\n",
      "cluster                  18.000000\n",
      "Name: 17850.0, dtype: float64\n",
      "\n",
      "## Step 2: Belongs to Cluster 18, which has these average characteristics:\n",
      "recency_days             18.033019\n",
      "frequency                16.410377\n",
      "total_spent            4425.028349\n",
      "unique_items            105.466981\n",
      "total_items            2494.221698\n",
      "avg_order_value         274.919524\n",
      "avg_items_per_order     156.697645\n",
      "avg_days_between          1.958521\n",
      "Name: 18, dtype: float64\n",
      "\n",
      "## Step 3: Top 3 Recommendations\n",
      "- JUMBO BAG RED RETROSPOT\n",
      "- REGENCY CAKESTAND 3 TIER\n",
      "- JUMBO BAG DOILEY PATTERNS\n"
     ]
    }
   ],
   "source": [
    "#Define the customer to analyze\n",
    "customer_id_to_check = 17850\n",
    "\n",
    "#Profile the Specific Customer\n",
    "print(f\"--- Analysis for Customer ID: {customer_id_to_check} ---\")\n",
    "user_profile = user_features.loc[customer_id_to_check]\n",
    "print(\"\\nCustomer's Personal Profile\")\n",
    "print(user_profile)\n",
    "\n",
    "#Understand Their Peer Group (Cluster)\n",
    "user_cluster_label = int(user_profile['cluster'])\n",
    "cluster_profile = cluster_analysis.loc[user_cluster_label]\n",
    "print(f\"\\nCustomer belongs to Cluster {user_cluster_label}, which has these average characteristics:\")\n",
    "print(cluster_profile)\n",
    "\n",
    "#Review Their Recommendations\n",
    "#Get recommended stock codes\n",
    "recommended_codes = get_recommendations(customer_id_to_check, top_n=3)\n",
    "\n",
    "#Translate codes to human-readable names\n",
    "recommended_items = [stock_code_map.get(code, \"Unknown Item\") for code in recommended_codes]\n",
    "\n",
    "print(\"\\nTop 3 Recommendations\")\n",
    "for item in recommended_items:\n",
    "    print(f\"- {item}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c08975d",
   "metadata": {},
   "source": [
    "This analysis reveals a successful, multi-layered segmentation where the model has correctly identified a high-value customer and provided relevant, if general, recommendations.\n",
    "\n",
    "### The Quality of the Segmentation\n",
    "\n",
    "The model correctly placed `CustomerID` 17850 into **Cluster 18**, a segment of \"power users.\" This grouping is logical because the customer's core purchasing habits—their high **`frequency`** (35) and **`total_spent`** ($5288)—are much more similar to the high average values of this cluster than to any other. While their spending is lower than the cluster's average ($10,235), they are still clearly in the same league, distinguishing them from casual, low-spending shoppers.\n",
    "\n",
    "### The \"Lapsed Power User\" Insight\n",
    "\n",
    "It is perfectly okay that the user is \"lapsed\" within this group; in fact, this is a crucial business insight. The cluster represents a **behavioral type** (\"power user\"), not a group where every member is identical. The model correctly determined that this user's history of high-volume, frequent purchasing makes them a power user. Their high **`recency`** (302 days vs. the group's average of 18) simply adds another layer to their profile: they are a power user **who is at risk of churning**. This is a highly valuable segment for a business to identify.\n",
    "\n",
    "### The Logic of the Recommendations\n",
    "\n",
    "The recommendations (\"JUMBO BAG,\" \"REGENCY CAKESTAND,\" etc.) are likely general best-sellers that are popular among a wide range of customers. We can assume these are logical recommendations for this cluster because this \"power user\" segment, which includes B2B customers and bulk buyers, naturally purchases a high volume of the most popular and useful items for resale or frequent use. Therefore, recommending the top-selling items to a member of this group is a safe and logical strategy to re-engage them with products their peers have found valuable.\n",
    "\n",
    "## Overall\n",
    "Throughout this project, we successfully guided the e-commerce dataset through a complete customer segmentation pipeline, from data cleaning and preparation to advanced feature engineering and unsupervised learning. By transforming raw transaction logs into rich user profiles using aggregation methods like RFM (Recency, Frequency, Monetary) analysis and then applying **K-Means clustering**, we partitioned the customer base into distinct, data-driven groups. The value of this process lies in the techniques themselves. The ability to engineer behavioral features and apply clustering to segment a user base is a fundamental skill in modern data analytics, essential for tackling real-world business challenges. This project has built a solid foundation in a new class of data science problems. Good work, give yourself a pat on the back."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "80cd1813",
   "metadata": {},
   "source": [
    "# Amazon Review Sentiment Analysis\n",
    "\n",
    "## Main Goals\n",
    "- Predict whether an Amazon review has a positive or negative sentiment.\n",
    "- Convert unstructured text data into meaningful numerical features.\n",
    "    - Clean and preprocess raw text from reviews.\n",
    "    - Apply TF-IDF vectorization to represent the text data.\n",
    "- Explore word-level features to understand what drives sentiment.\n",
    "- Compare and analyze results from two different classification models.\n",
    "\n",
    "### Context\n",
    "In the modern digital marketplace, customer reviews are a cornerstone of consumer decision-making and a vital source of feedback for businesses. Understanding the sentiment expressed in these reviews at a large scale presents a significant challenge. Automatically classifying reviews as positive or negative is crucial for businesses to gauge customer satisfaction, identify product strengths and weaknesses, and manage their brand reputation. In the field of data science, Natural Language Processing (NLP) offers a robust toolkit for converting unstructured text into features for predictive modeling. This project leverages a real-world dataset of past Amazon reviews to build a classification model that predicts sentiment, enabling a more data-driven approach to understanding the voice of the customer.\n",
    "\n",
    "## 1. Loading in the Data\n",
    "For this project, we will use the [Amazon Fine Food Reviews](https://www.kaggle.com/datasets/snap/amazon-fine-food-reviews?select=Reviews.csv) from Kaggle. In accordance with Kaggle licenses, please directly visit the Kaggle website and download the `Reviews.csv` dataset for this activity, and then upload the file to the same directory as the notebook file.\n",
    "\n",
    "We can start by loading in the dataset into a pandas dataframe, and then displaying it to ensure it loaded correctly, and so we can see what the features are and how the target is displayed. This means that we have to start by importing pandas as well.\n",
    "\n",
    "It's worth mentioning that anytime you have a dataset from an external source, such as Kaggle, you can and should refer back to the source of the data to clear up misconceptions and also to get a better understanding of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7255f35d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>UserId</th>\n",
       "      <th>ProfileName</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>Score</th>\n",
       "      <th>Time</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>B001E4KFG0</td>\n",
       "      <td>A3SGXH7AUHU8GW</td>\n",
       "      <td>delmartian</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1303862400</td>\n",
       "      <td>Good Quality Dog Food</td>\n",
       "      <td>I have bought several of the Vitality canned d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>B00813GRG4</td>\n",
       "      <td>A1D87F6ZCVE5NK</td>\n",
       "      <td>dll pa</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1346976000</td>\n",
       "      <td>Not as Advertised</td>\n",
       "      <td>Product arrived labeled as Jumbo Salted Peanut...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>B000LQOCH0</td>\n",
       "      <td>ABXLMWJIXXAIN</td>\n",
       "      <td>Natalia Corres \"Natalia Corres\"</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1219017600</td>\n",
       "      <td>\"Delight\" says it all</td>\n",
       "      <td>This is a confection that has been around a fe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>B000UA0QIQ</td>\n",
       "      <td>A395BORC6FGVXV</td>\n",
       "      <td>Karl</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1307923200</td>\n",
       "      <td>Cough Medicine</td>\n",
       "      <td>If you are looking for the secret ingredient i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>B006K2ZZ7K</td>\n",
       "      <td>A1UQRSCLF8GW1T</td>\n",
       "      <td>Michael D. Bigham \"M. Wassir\"</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1350777600</td>\n",
       "      <td>Great taffy</td>\n",
       "      <td>Great taffy at a great price.  There was a wid...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568449</th>\n",
       "      <td>568450</td>\n",
       "      <td>B001EO7N10</td>\n",
       "      <td>A28KG5XORO54AY</td>\n",
       "      <td>Lettie D. Carter</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1299628800</td>\n",
       "      <td>Will not do without</td>\n",
       "      <td>Great for sesame chicken..this is a good if no...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568450</th>\n",
       "      <td>568451</td>\n",
       "      <td>B003S1WTCU</td>\n",
       "      <td>A3I8AFVPEE8KI5</td>\n",
       "      <td>R. Sawyer</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1331251200</td>\n",
       "      <td>disappointed</td>\n",
       "      <td>I'm disappointed with the flavor. The chocolat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568451</th>\n",
       "      <td>568452</td>\n",
       "      <td>B004I613EE</td>\n",
       "      <td>A121AA1GQV751Z</td>\n",
       "      <td>pksd \"pk_007\"</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>1329782400</td>\n",
       "      <td>Perfect for our maltipoo</td>\n",
       "      <td>These stars are small, so you can give 10-15 o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568452</th>\n",
       "      <td>568453</td>\n",
       "      <td>B004I613EE</td>\n",
       "      <td>A3IBEVCTXKNOH</td>\n",
       "      <td>Kathy A. Welch \"katwel\"</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1331596800</td>\n",
       "      <td>Favorite Training and reward treat</td>\n",
       "      <td>These are the BEST treats for training and rew...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568453</th>\n",
       "      <td>568454</td>\n",
       "      <td>B001LR2CU2</td>\n",
       "      <td>A3LGQPJCZVL9UC</td>\n",
       "      <td>srfell17</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1338422400</td>\n",
       "      <td>Great Honey</td>\n",
       "      <td>I am very satisfied ,product is as advertised,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>568454 rows Ã— 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Id   ProductId          UserId                      ProfileName  \\\n",
       "0            1  B001E4KFG0  A3SGXH7AUHU8GW                       delmartian   \n",
       "1            2  B00813GRG4  A1D87F6ZCVE5NK                           dll pa   \n",
       "2            3  B000LQOCH0   ABXLMWJIXXAIN  Natalia Corres \"Natalia Corres\"   \n",
       "3            4  B000UA0QIQ  A395BORC6FGVXV                             Karl   \n",
       "4            5  B006K2ZZ7K  A1UQRSCLF8GW1T    Michael D. Bigham \"M. Wassir\"   \n",
       "...        ...         ...             ...                              ...   \n",
       "568449  568450  B001EO7N10  A28KG5XORO54AY                 Lettie D. Carter   \n",
       "568450  568451  B003S1WTCU  A3I8AFVPEE8KI5                        R. Sawyer   \n",
       "568451  568452  B004I613EE  A121AA1GQV751Z                    pksd \"pk_007\"   \n",
       "568452  568453  B004I613EE   A3IBEVCTXKNOH          Kathy A. Welch \"katwel\"   \n",
       "568453  568454  B001LR2CU2  A3LGQPJCZVL9UC                         srfell17   \n",
       "\n",
       "        HelpfulnessNumerator  HelpfulnessDenominator  Score        Time  \\\n",
       "0                          1                       1      5  1303862400   \n",
       "1                          0                       0      1  1346976000   \n",
       "2                          1                       1      4  1219017600   \n",
       "3                          3                       3      2  1307923200   \n",
       "4                          0                       0      5  1350777600   \n",
       "...                      ...                     ...    ...         ...   \n",
       "568449                     0                       0      5  1299628800   \n",
       "568450                     0                       0      2  1331251200   \n",
       "568451                     2                       2      5  1329782400   \n",
       "568452                     1                       1      5  1331596800   \n",
       "568453                     0                       0      5  1338422400   \n",
       "\n",
       "                                   Summary  \\\n",
       "0                    Good Quality Dog Food   \n",
       "1                        Not as Advertised   \n",
       "2                    \"Delight\" says it all   \n",
       "3                           Cough Medicine   \n",
       "4                              Great taffy   \n",
       "...                                    ...   \n",
       "568449                 Will not do without   \n",
       "568450                        disappointed   \n",
       "568451            Perfect for our maltipoo   \n",
       "568452  Favorite Training and reward treat   \n",
       "568453                         Great Honey   \n",
       "\n",
       "                                                     Text  \n",
       "0       I have bought several of the Vitality canned d...  \n",
       "1       Product arrived labeled as Jumbo Salted Peanut...  \n",
       "2       This is a confection that has been around a fe...  \n",
       "3       If you are looking for the secret ingredient i...  \n",
       "4       Great taffy at a great price.  There was a wid...  \n",
       "...                                                   ...  \n",
       "568449  Great for sesame chicken..this is a good if no...  \n",
       "568450  I'm disappointed with the flavor. The chocolat...  \n",
       "568451  These stars are small, so you can give 10-15 o...  \n",
       "568452  These are the BEST treats for training and rew...  \n",
       "568453  I am very satisfied ,product is as advertised,...  \n",
       "\n",
       "[568454 rows x 10 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Import pandas\n",
    "import pandas as pd\n",
    "\n",
    "#Read the CSV file\n",
    "df = pd.read_csv('Reviews.csv')\n",
    "\n",
    "#Display the dataframe\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71217389",
   "metadata": {},
   "source": [
    "Using display() on our data shows us our features. Let's list out some of the features worth noting and clarifying. We'll use infromation from Kaggle to supplement what displaying the info tells us as well.\n",
    "\n",
    "- HelpfulnessNumerator: Number of users who found the review helpful\n",
    "- HelpfulnessDenominator: Number of users who indicated whether they found the review helpful or not \n",
    "- Score: The ratimg for the product on a scale of 1 to 5. \n",
    "- Time: While it should simply just be the time of the review, the entries might look unfamiliar. The data is in Unix timestamp (also known as epoch time), which records time as the number of seconds since January 1st, 1970. Something like 1303862400 would be the same as 2011-04-27 00:00:00.\n",
    "- Text: This is just the text of the reviews, and we'll use this in combination with other features to train our model on sentiment\n",
    "\n",
    "Note that we don't have a clear sentiment target, so when the time for feature engineering comes, we'll take what data we have now and create a binary target the strictly tells our model whether a review is postive or negative.\n",
    "\n",
    "As such, we're ready to move on to preproccessing our data.\n",
    "\n",
    "## 2. Preprocessing\n",
    "\n",
    "Let's now start to clean up our data. We do want to make it as easy as possible for our model to read our data.\n",
    "\n",
    "### Handling Null Values\n",
    "When preprocessing data, a good place to start is with handling null entires. They're easy to check, but leaving them in can cause major issues for our model down the line. Thankfully, pandas has plenty of tools for us to use to check if we have any null values. Something to note is that since we are handling a lot of text, there is a chance that instead of being considered null in the data, it might just be a blank string. As such, we'll check for blank strings as well. We'll use a bit more code than usual for this, so do follow along.\n",
    "\n",
    "Let's start by printing out the null values in the data frame. In a different code cell, we can check for blank strings in specific columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c73436e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Id                         0\n",
      "ProductId                  0\n",
      "UserId                     0\n",
      "ProfileName               26\n",
      "HelpfulnessNumerator       0\n",
      "HelpfulnessDenominator     0\n",
      "Score                      0\n",
      "Time                       0\n",
      "Summary                   27\n",
      "Text                       0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#Check for null values\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17b43ba6",
   "metadata": {},
   "source": [
    "Here we can see that we have 27 null entries in the ProfileName and Summary features. Before handling these rows, let's continue with what we previously said, and check for blank strings as well, as they wouldn't have been counted with the null entries we just saw."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0263191b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing String Columns for Missing or Empty Values\n",
      "\n",
      "'Summary':\n",
      "Empty strings: 0\n",
      "Whitespace-only strings: 0\n",
      "\n",
      "'Text':\n",
      "Empty strings: 0\n",
      "Whitespace-only strings: 0\n",
      "\n",
      "'ProductId':\n",
      "Empty strings: 0\n",
      "Whitespace-only strings: 0\n",
      "\n",
      "'UserId':\n",
      "Empty strings: 0\n",
      "Whitespace-only strings: 0\n",
      "\n",
      "'ProfileName':\n",
      "Empty strings: 0\n",
      "Whitespace-only strings: 0\n"
     ]
    }
   ],
   "source": [
    "#List of the object/string columns we want to inspect\n",
    "columns_to_check = [\n",
    "    'Summary',\n",
    "    'Text',\n",
    "    'ProductId',\n",
    "    'UserId',\n",
    "    'ProfileName'\n",
    "]\n",
    "\n",
    "print(\"Analyzing String Columns for Missing or Empty Values\")\n",
    "\n",
    "# Loop through each column in our list\n",
    "for col in columns_to_check:\n",
    "    #Print the column name for clarity\n",
    "    print(f\"\\n'{col}':\")\n",
    "\n",
    "    #Check for empty strings ('')\n",
    "    #This comparison might not work if the column has NaN values,so we handle that with fillna.\n",
    "    #This doesn't fill the actual Dataframe and remove nulls, it just stores what it would be if we did.\n",
    "    empty_strings = (df[col] == '').fillna('').sum()\n",
    "    print(f\"Empty strings: {empty_strings}\")\n",
    "    \n",
    "\n",
    "    #Check for whitespace-only strings ('   ')\n",
    "    #We fill potential NaN values with an empty string first so the .str accessor doesn't cause an error.\n",
    "    whitespace_strings = df[col].fillna('').str.isspace().sum()\n",
    "    print(f\"Whitespace-only strings: {whitespace_strings}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2031b6e5",
   "metadata": {},
   "source": [
    "Fortunately for us, our data doesn't come with empty strings or whitespace strings, so let's just handle the original null entries we saw in the ProfileName and Summary features. Something important to note is that while we are missing these values, we aren't missing other important information in the same row, such as the text or scores. As such, we can fill them in. It doesn't really matter what we fill them in with, as long as it's a string. The best practice however, is to fill them in with empty strings, so we'll do just that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3bdf4cb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary        0\n",
      "ProfileName    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#Fill the nulls with empty strings\n",
    "df['Summary'] = df['Summary'].fillna('')\n",
    "df['ProfileName'] = df['ProfileName'].fillna('')\n",
    "\n",
    "#You can then verify the nulls are gone:\n",
    "print(df[['Summary', 'ProfileName']].isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d5db025",
   "metadata": {},
   "source": [
    "### Removing Nuetrality \n",
    "Removing reviews with a 3-star rating is a deliberate strategic decision to improve the quality and clarity of our data. These neutral reviews are often ambiguous, containing a mix of positive and negative language that makes it difficult for a model to learn a clear signal. By dropping them, we create a distinct binary problem with clearly positive (4-5 stars) and clearly negative (1-2 stars) classes. While this does reduce the overall quantity of our data, it significantly improves the quality, which allows models using techniques like Bag-of-Words and TF-IDF to more easily identify the words and phrases strongly associated with each sentiment, leading to a more robust and better-performing final model.\n",
    "\n",
    "Note that while this simplification is a great technique for building a strong binary classifier, it isn't always the right approach. For more complex, real-world applications, keeping the neutral class can provide a more nuanced understanding of customer feedback. In those cases, it would be ideal to use a more advanced model, such as a neural network, which is better equipped to handle the subtlety and complexity of a three-class (positive, neutral, negative) sentiment problem. For our current purpose of learning TF-IDF later on, however, the binary approach is better suited.\n",
    "\n",
    "With that in mind, let's go ahead and remove rows where the score is 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "035ca77f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>UserId</th>\n",
       "      <th>ProfileName</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>Score</th>\n",
       "      <th>Time</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>B001E4KFG0</td>\n",
       "      <td>A3SGXH7AUHU8GW</td>\n",
       "      <td>delmartian</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1303862400</td>\n",
       "      <td>Good Quality Dog Food</td>\n",
       "      <td>I have bought several of the Vitality canned d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>B00813GRG4</td>\n",
       "      <td>A1D87F6ZCVE5NK</td>\n",
       "      <td>dll pa</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1346976000</td>\n",
       "      <td>Not as Advertised</td>\n",
       "      <td>Product arrived labeled as Jumbo Salted Peanut...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>B000LQOCH0</td>\n",
       "      <td>ABXLMWJIXXAIN</td>\n",
       "      <td>Natalia Corres \"Natalia Corres\"</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1219017600</td>\n",
       "      <td>\"Delight\" says it all</td>\n",
       "      <td>This is a confection that has been around a fe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>B000UA0QIQ</td>\n",
       "      <td>A395BORC6FGVXV</td>\n",
       "      <td>Karl</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1307923200</td>\n",
       "      <td>Cough Medicine</td>\n",
       "      <td>If you are looking for the secret ingredient i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>B006K2ZZ7K</td>\n",
       "      <td>A1UQRSCLF8GW1T</td>\n",
       "      <td>Michael D. Bigham \"M. Wassir\"</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1350777600</td>\n",
       "      <td>Great taffy</td>\n",
       "      <td>Great taffy at a great price.  There was a wid...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568449</th>\n",
       "      <td>568450</td>\n",
       "      <td>B001EO7N10</td>\n",
       "      <td>A28KG5XORO54AY</td>\n",
       "      <td>Lettie D. Carter</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1299628800</td>\n",
       "      <td>Will not do without</td>\n",
       "      <td>Great for sesame chicken..this is a good if no...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568450</th>\n",
       "      <td>568451</td>\n",
       "      <td>B003S1WTCU</td>\n",
       "      <td>A3I8AFVPEE8KI5</td>\n",
       "      <td>R. Sawyer</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1331251200</td>\n",
       "      <td>disappointed</td>\n",
       "      <td>I'm disappointed with the flavor. The chocolat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568451</th>\n",
       "      <td>568452</td>\n",
       "      <td>B004I613EE</td>\n",
       "      <td>A121AA1GQV751Z</td>\n",
       "      <td>pksd \"pk_007\"</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>1329782400</td>\n",
       "      <td>Perfect for our maltipoo</td>\n",
       "      <td>These stars are small, so you can give 10-15 o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568452</th>\n",
       "      <td>568453</td>\n",
       "      <td>B004I613EE</td>\n",
       "      <td>A3IBEVCTXKNOH</td>\n",
       "      <td>Kathy A. Welch \"katwel\"</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1331596800</td>\n",
       "      <td>Favorite Training and reward treat</td>\n",
       "      <td>These are the BEST treats for training and rew...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568453</th>\n",
       "      <td>568454</td>\n",
       "      <td>B001LR2CU2</td>\n",
       "      <td>A3LGQPJCZVL9UC</td>\n",
       "      <td>srfell17</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1338422400</td>\n",
       "      <td>Great Honey</td>\n",
       "      <td>I am very satisfied ,product is as advertised,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>525814 rows Ã— 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Id   ProductId          UserId                      ProfileName  \\\n",
       "0            1  B001E4KFG0  A3SGXH7AUHU8GW                       delmartian   \n",
       "1            2  B00813GRG4  A1D87F6ZCVE5NK                           dll pa   \n",
       "2            3  B000LQOCH0   ABXLMWJIXXAIN  Natalia Corres \"Natalia Corres\"   \n",
       "3            4  B000UA0QIQ  A395BORC6FGVXV                             Karl   \n",
       "4            5  B006K2ZZ7K  A1UQRSCLF8GW1T    Michael D. Bigham \"M. Wassir\"   \n",
       "...        ...         ...             ...                              ...   \n",
       "568449  568450  B001EO7N10  A28KG5XORO54AY                 Lettie D. Carter   \n",
       "568450  568451  B003S1WTCU  A3I8AFVPEE8KI5                        R. Sawyer   \n",
       "568451  568452  B004I613EE  A121AA1GQV751Z                    pksd \"pk_007\"   \n",
       "568452  568453  B004I613EE   A3IBEVCTXKNOH          Kathy A. Welch \"katwel\"   \n",
       "568453  568454  B001LR2CU2  A3LGQPJCZVL9UC                         srfell17   \n",
       "\n",
       "        HelpfulnessNumerator  HelpfulnessDenominator  Score        Time  \\\n",
       "0                          1                       1      5  1303862400   \n",
       "1                          0                       0      1  1346976000   \n",
       "2                          1                       1      4  1219017600   \n",
       "3                          3                       3      2  1307923200   \n",
       "4                          0                       0      5  1350777600   \n",
       "...                      ...                     ...    ...         ...   \n",
       "568449                     0                       0      5  1299628800   \n",
       "568450                     0                       0      2  1331251200   \n",
       "568451                     2                       2      5  1329782400   \n",
       "568452                     1                       1      5  1331596800   \n",
       "568453                     0                       0      5  1338422400   \n",
       "\n",
       "                                   Summary  \\\n",
       "0                    Good Quality Dog Food   \n",
       "1                        Not as Advertised   \n",
       "2                    \"Delight\" says it all   \n",
       "3                           Cough Medicine   \n",
       "4                              Great taffy   \n",
       "...                                    ...   \n",
       "568449                 Will not do without   \n",
       "568450                        disappointed   \n",
       "568451            Perfect for our maltipoo   \n",
       "568452  Favorite Training and reward treat   \n",
       "568453                         Great Honey   \n",
       "\n",
       "                                                     Text  \n",
       "0       I have bought several of the Vitality canned d...  \n",
       "1       Product arrived labeled as Jumbo Salted Peanut...  \n",
       "2       This is a confection that has been around a fe...  \n",
       "3       If you are looking for the secret ingredient i...  \n",
       "4       Great taffy at a great price.  There was a wid...  \n",
       "...                                                   ...  \n",
       "568449  Great for sesame chicken..this is a good if no...  \n",
       "568450  I'm disappointed with the flavor. The chocolat...  \n",
       "568451  These stars are small, so you can give 10-15 o...  \n",
       "568452  These are the BEST treats for training and rew...  \n",
       "568453  I am very satisfied ,product is as advertised,...  \n",
       "\n",
       "[525814 rows x 10 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Keep only rows where the score is not 3\n",
    "df = df[df['Score'] != 3].copy()\n",
    "\n",
    "#Display the filtered DataFrame\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0407586a",
   "metadata": {},
   "source": [
    "It might be difficult to see any changes on the surface, but you'll notice that we did lose nearly 40,000 rows. Getting rid of data like this isn't always ideal, so do make sure you have a large enough dataset, and valid reason, before making such a decision. \n",
    "\n",
    "### Defining Sentiment \n",
    "\n",
    "As of right now, we have a lot of indicators of sentiment, but nothing that clearly defines and categorizes certain reviews as positive or negative. As such, we'll take the liberty of doing so. This will be our target as well. We'll say that anything with a score greater than a 3 is positive, and anything with below a 3 is negative. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d58548b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Score</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568449</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568450</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568451</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568452</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568453</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>525814 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Score  sentiment\n",
       "0           5          1\n",
       "1           1          0\n",
       "2           4          1\n",
       "3           2          0\n",
       "4           5          1\n",
       "...       ...        ...\n",
       "568449      5          1\n",
       "568450      2          0\n",
       "568451      5          1\n",
       "568452      5          1\n",
       "568453      5          1\n",
       "\n",
       "[525814 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create the sentiment column: 1 for positive, 0 for negative\n",
    "df['sentiment'] = df['Score'].apply(lambda score: 1 if score > 3 else 0)\n",
    "\n",
    "#Display the DataFrame with the new sentiment column\n",
    "display(df[['Score', 'sentiment']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aadf6a33",
   "metadata": {},
   "source": [
    "Great! With that, we have our target. Many models such as logistic regressions or random forests excel at predicting binary targets, so we have options.\n",
    "\n",
    "### Handling Duplicates\n",
    "While not obvious from simply inspecting the dataframe as we have done so far, looking at the Kaggle documentation for this dataset reveals that even though we have over 500,000 rows, we only had 393579 unique reviews. Without reviews scored at 3 stars, this number is likely even lower. On Amazon, certain products are bunched together, typically by brand as a different version of the same product. Any reviews on one of those products are also visible on a different version of the same product, which is likely what is causing our duplication issue. For the sake of our model reading everything properly, we'll be filtering out these duplicates.\n",
    "\n",
    "Fortunately, our work is cut out for us, as pandas does have a drop_duplicates function that does exactly what we need it to. We'll be specifying the columns when we use this function as well, since the review, time, profile, and user Id are all likely duplicates, features like the product Id are likely to be unique all throughout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "986982f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>UserId</th>\n",
       "      <th>ProfileName</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>Score</th>\n",
       "      <th>Time</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>B001E4KFG0</td>\n",
       "      <td>A3SGXH7AUHU8GW</td>\n",
       "      <td>delmartian</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1303862400</td>\n",
       "      <td>Good Quality Dog Food</td>\n",
       "      <td>I have bought several of the Vitality canned d...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>B00813GRG4</td>\n",
       "      <td>A1D87F6ZCVE5NK</td>\n",
       "      <td>dll pa</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1346976000</td>\n",
       "      <td>Not as Advertised</td>\n",
       "      <td>Product arrived labeled as Jumbo Salted Peanut...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>B000LQOCH0</td>\n",
       "      <td>ABXLMWJIXXAIN</td>\n",
       "      <td>Natalia Corres \"Natalia Corres\"</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1219017600</td>\n",
       "      <td>\"Delight\" says it all</td>\n",
       "      <td>This is a confection that has been around a fe...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>B000UA0QIQ</td>\n",
       "      <td>A395BORC6FGVXV</td>\n",
       "      <td>Karl</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1307923200</td>\n",
       "      <td>Cough Medicine</td>\n",
       "      <td>If you are looking for the secret ingredient i...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>B006K2ZZ7K</td>\n",
       "      <td>A1UQRSCLF8GW1T</td>\n",
       "      <td>Michael D. Bigham \"M. Wassir\"</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1350777600</td>\n",
       "      <td>Great taffy</td>\n",
       "      <td>Great taffy at a great price.  There was a wid...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568449</th>\n",
       "      <td>568450</td>\n",
       "      <td>B001EO7N10</td>\n",
       "      <td>A28KG5XORO54AY</td>\n",
       "      <td>Lettie D. Carter</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1299628800</td>\n",
       "      <td>Will not do without</td>\n",
       "      <td>Great for sesame chicken..this is a good if no...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568450</th>\n",
       "      <td>568451</td>\n",
       "      <td>B003S1WTCU</td>\n",
       "      <td>A3I8AFVPEE8KI5</td>\n",
       "      <td>R. Sawyer</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1331251200</td>\n",
       "      <td>disappointed</td>\n",
       "      <td>I'm disappointed with the flavor. The chocolat...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568451</th>\n",
       "      <td>568452</td>\n",
       "      <td>B004I613EE</td>\n",
       "      <td>A121AA1GQV751Z</td>\n",
       "      <td>pksd \"pk_007\"</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>1329782400</td>\n",
       "      <td>Perfect for our maltipoo</td>\n",
       "      <td>These stars are small, so you can give 10-15 o...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568452</th>\n",
       "      <td>568453</td>\n",
       "      <td>B004I613EE</td>\n",
       "      <td>A3IBEVCTXKNOH</td>\n",
       "      <td>Kathy A. Welch \"katwel\"</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1331596800</td>\n",
       "      <td>Favorite Training and reward treat</td>\n",
       "      <td>These are the BEST treats for training and rew...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568453</th>\n",
       "      <td>568454</td>\n",
       "      <td>B001LR2CU2</td>\n",
       "      <td>A3LGQPJCZVL9UC</td>\n",
       "      <td>srfell17</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1338422400</td>\n",
       "      <td>Great Honey</td>\n",
       "      <td>I am very satisfied ,product is as advertised,...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>364173 rows Ã— 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Id   ProductId          UserId                      ProfileName  \\\n",
       "0            1  B001E4KFG0  A3SGXH7AUHU8GW                       delmartian   \n",
       "1            2  B00813GRG4  A1D87F6ZCVE5NK                           dll pa   \n",
       "2            3  B000LQOCH0   ABXLMWJIXXAIN  Natalia Corres \"Natalia Corres\"   \n",
       "3            4  B000UA0QIQ  A395BORC6FGVXV                             Karl   \n",
       "4            5  B006K2ZZ7K  A1UQRSCLF8GW1T    Michael D. Bigham \"M. Wassir\"   \n",
       "...        ...         ...             ...                              ...   \n",
       "568449  568450  B001EO7N10  A28KG5XORO54AY                 Lettie D. Carter   \n",
       "568450  568451  B003S1WTCU  A3I8AFVPEE8KI5                        R. Sawyer   \n",
       "568451  568452  B004I613EE  A121AA1GQV751Z                    pksd \"pk_007\"   \n",
       "568452  568453  B004I613EE   A3IBEVCTXKNOH          Kathy A. Welch \"katwel\"   \n",
       "568453  568454  B001LR2CU2  A3LGQPJCZVL9UC                         srfell17   \n",
       "\n",
       "        HelpfulnessNumerator  HelpfulnessDenominator  Score        Time  \\\n",
       "0                          1                       1      5  1303862400   \n",
       "1                          0                       0      1  1346976000   \n",
       "2                          1                       1      4  1219017600   \n",
       "3                          3                       3      2  1307923200   \n",
       "4                          0                       0      5  1350777600   \n",
       "...                      ...                     ...    ...         ...   \n",
       "568449                     0                       0      5  1299628800   \n",
       "568450                     0                       0      2  1331251200   \n",
       "568451                     2                       2      5  1329782400   \n",
       "568452                     1                       1      5  1331596800   \n",
       "568453                     0                       0      5  1338422400   \n",
       "\n",
       "                                   Summary  \\\n",
       "0                    Good Quality Dog Food   \n",
       "1                        Not as Advertised   \n",
       "2                    \"Delight\" says it all   \n",
       "3                           Cough Medicine   \n",
       "4                              Great taffy   \n",
       "...                                    ...   \n",
       "568449                 Will not do without   \n",
       "568450                        disappointed   \n",
       "568451            Perfect for our maltipoo   \n",
       "568452  Favorite Training and reward treat   \n",
       "568453                         Great Honey   \n",
       "\n",
       "                                                     Text  sentiment  \n",
       "0       I have bought several of the Vitality canned d...          1  \n",
       "1       Product arrived labeled as Jumbo Salted Peanut...          0  \n",
       "2       This is a confection that has been around a fe...          1  \n",
       "3       If you are looking for the secret ingredient i...          0  \n",
       "4       Great taffy at a great price.  There was a wid...          1  \n",
       "...                                                   ...        ...  \n",
       "568449  Great for sesame chicken..this is a good if no...          1  \n",
       "568450  I'm disappointed with the flavor. The chocolat...          0  \n",
       "568451  These stars are small, so you can give 10-15 o...          1  \n",
       "568452  These are the BEST treats for training and rew...          1  \n",
       "568453  I am very satisfied ,product is as advertised,...          1  \n",
       "\n",
       "[364173 rows x 11 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Drop duplicates based on the user and their review text\n",
    "df = df.drop_duplicates(subset=['UserId', 'ProfileName', 'Time', 'Text'])\n",
    "\n",
    "#Display the DataFrame after dropping duplicates to see how many rows were removed\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ec0b96",
   "metadata": {},
   "source": [
    "While losing over 100,000 entries doesn't sound good, it's important to remember that we did this so that our model doesn't get confused with duplicate entries. \n",
    "\n",
    "### Dropping Noisy Features\n",
    "\n",
    "To further clean our data, we'll be taking a look at certain features once again. Our goal is to have this data ready in a form that is easy for a model to learn from. Features such as the Id's and the profile name are likely not going to help our model learn the data. In fact, it might even cause data leakage, as it can assume that if a person or item tends to have negative or positive reviews, it can then guess based on either the person or item. When confronted with a new item being reveiwed by a new person, then it'll have trouble making a proper prediction.\n",
    "\n",
    "We'll also be dropping our HelpfulnessNumerator and HelpfulNessDenominator features out of worry of data leakage. How helpful a review to others is simply won't be available to us when reading a new review, or at least when inputing a new review into the model.\n",
    "\n",
    "As such, we'll be droping the Id columns, as well as the profile name column. This includes the regular Id column, since although it simply acts as an index, it still doesn't help our model in the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dfa965d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>Score</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>Good Quality Dog Food</td>\n",
       "      <td>I have bought several of the Vitality canned d...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Not as Advertised</td>\n",
       "      <td>Product arrived labeled as Jumbo Salted Peanut...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>\"Delight\" says it all</td>\n",
       "      <td>This is a confection that has been around a fe...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>Cough Medicine</td>\n",
       "      <td>If you are looking for the secret ingredient i...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>Great taffy</td>\n",
       "      <td>Great taffy at a great price.  There was a wid...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568449</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>Will not do without</td>\n",
       "      <td>Great for sesame chicken..this is a good if no...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568450</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>disappointed</td>\n",
       "      <td>I'm disappointed with the flavor. The chocolat...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568451</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>Perfect for our maltipoo</td>\n",
       "      <td>These stars are small, so you can give 10-15 o...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568452</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>Favorite Training and reward treat</td>\n",
       "      <td>These are the BEST treats for training and rew...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568453</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>Great Honey</td>\n",
       "      <td>I am very satisfied ,product is as advertised,...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>364173 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        HelpfulnessNumerator  HelpfulnessDenominator  Score  \\\n",
       "0                          1                       1      5   \n",
       "1                          0                       0      1   \n",
       "2                          1                       1      4   \n",
       "3                          3                       3      2   \n",
       "4                          0                       0      5   \n",
       "...                      ...                     ...    ...   \n",
       "568449                     0                       0      5   \n",
       "568450                     0                       0      2   \n",
       "568451                     2                       2      5   \n",
       "568452                     1                       1      5   \n",
       "568453                     0                       0      5   \n",
       "\n",
       "                                   Summary  \\\n",
       "0                    Good Quality Dog Food   \n",
       "1                        Not as Advertised   \n",
       "2                    \"Delight\" says it all   \n",
       "3                           Cough Medicine   \n",
       "4                              Great taffy   \n",
       "...                                    ...   \n",
       "568449                 Will not do without   \n",
       "568450                        disappointed   \n",
       "568451            Perfect for our maltipoo   \n",
       "568452  Favorite Training and reward treat   \n",
       "568453                         Great Honey   \n",
       "\n",
       "                                                     Text  sentiment  \n",
       "0       I have bought several of the Vitality canned d...          1  \n",
       "1       Product arrived labeled as Jumbo Salted Peanut...          0  \n",
       "2       This is a confection that has been around a fe...          1  \n",
       "3       If you are looking for the secret ingredient i...          0  \n",
       "4       Great taffy at a great price.  There was a wid...          1  \n",
       "...                                                   ...        ...  \n",
       "568449  Great for sesame chicken..this is a good if no...          1  \n",
       "568450  I'm disappointed with the flavor. The chocolat...          0  \n",
       "568451  These stars are small, so you can give 10-15 o...          1  \n",
       "568452  These are the BEST treats for training and rew...          1  \n",
       "568453  I am very satisfied ,product is as advertised,...          1  \n",
       "\n",
       "[364173 rows x 6 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Drop our noisy columns\n",
    "#We drop time too, since we don't want bias for time of reviews.\n",
    "#Note that if we were analyzing something like sentiment over seasons, we would need it.\n",
    "df = df.drop(columns=['Id', 'ProductId', 'UserId', 'Time','ProfileName'])\n",
    "\n",
    "#View our changes\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "545a23c1",
   "metadata": {},
   "source": [
    "### Create a balanced dataset\n",
    "\n",
    "Another important step of preprocessing we have to perform is balancing the data. As it is right now, we have significantly more postive reviews than negative reviews. Simply feeding this in to our model might cause it to simply guess positive by default, as it would just be the safe option. Our solution to this is to create a balanced dataset where we have an equal amount of both positive and negative reviews. While this does mean discarded even more reviews, note that we still have ample data to work with and to train out model with, especially since it is text data. However, with much smaller datasets, steps like these might be detrimental, so do be cautious before doing so. In our case, it'll help our model significantly.\n",
    "\n",
    "Let's start by creating two dataframes for the possible sentiments, and then taking a sample of the positve reviews. We'll then concatenate them together to create our new balanced dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26895c3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Dataset Balancing Complete ---\n",
      "Original positive reviews: 307063\n",
      "Original negative reviews: 57110\n",
      "\n",
      "--- New Balanced Dataset ---\n",
      "sentiment\n",
      "0    57110\n",
      "1    57110\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#Separate the reviews into two DataFrames: one for positive, one for negative.\n",
    "df_positive = df[df['sentiment'] == 1]\n",
    "df_negative = df[df['sentiment'] == 0]\n",
    "\n",
    "#Randomly sample the positive reviews to match the number of negative reviews.\n",
    "#This takes a random subset of the majority class.\n",
    "df_positive_downsampled = df_positive.sample(n=len(df_negative), random_state=64)\n",
    "\n",
    "#Concatenate the negative reviews and the downsampled positive reviews back together.\n",
    "df_balanced = pd.concat([df_positive_downsampled, df_negative])\n",
    "\n",
    "#Shuffle the balanced DataFrame to mix the rows up. Not really necessary, but good practice.\n",
    "#We also reassign this balanced DataFrame to our original Dataframe for consistency.\n",
    "df = df_balanced.sample(frac=1, random_state=64).reset_index(drop=True)\n",
    "\n",
    "#Just to show and compare the original and new balanced datasets\n",
    "print(f\"Original positive reviews: {len(df_positive)}\")\n",
    "print(f\"Original negative reviews: {len(df_negative)}\")\n",
    "print(\"\\nNew Balanced Dataset:\")\n",
    "print(df['sentiment'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23dd2a6e",
   "metadata": {},
   "source": [
    "Here we can see, while we previously had nearly 5-6 times as many postive reviews as negative reviews, we now have an equal amount of postive and negative reviews. When the time comes to train the model, it'll perform much better.\n",
    "\n",
    "## 3. Text Cleaning \n",
    "\n",
    "The next essential step in our project is to perform text cleaning. The main goal here is to standardize our raw review text by removing all the noise that doesn't help in determining sentiment. This ensures that when our model analyzes the data, it focuses only on what's important. If we feed it messy data, we'll get a messy, unreliable model. To do this, we'll create a single function that performs a few key cleaning steps on each review. \n",
    "\n",
    "Note that for this function we'll be combining our summary and text features, as while summary may not contain many words, the few words that are contained end up being impactful. We'll be applying the same cleaning strategy to the combined feature.\n",
    "\n",
    "First, it will remove any stray HTML tags. While it might seem odd to check for such things in Amazon reviews, there's a chance that the webscraper or whatever tool used to collect the reviews did so by pulling in the raw HTML data that contained the text. This might be something like `<br />` to signify a paragraph break on the review in the website. We might not have directly seen any HTML tags, but this is a good check to have.\n",
    "\n",
    "Next, it will convert all text to lowercase so that words like \"Good\", \"good\", and \"GOOD\" are treated as the same word. It will also strip out all punctuation so that words like \"bad\" and \"bad!\" are also treated the same.\n",
    "\n",
    "Typically this process would also include dealing with \"stop words\", which are common, unimportant words such as \"the\", \"a\", \"is\", and \"in\". This would allow the model to pay attention to more meaningful words. However, and fortunately for us, the skicit-learn library has a list of stop words saved for this exact reason, so we won't have to worry about getting those right now.\n",
    "\n",
    "For our cleaning function, we'll also import Python's built-in re module to handle tasks that require Regular Expressions, often called \"regex.\" Think of regex as a powerful tool for finding and replacing specific patterns within text, much like a supercharged \"Find & Replace.\" We use it for two key jobs: first, to instantly find and remove any HTML tags by searching for the pattern of text enclosed in angle brackets. Second, we use it to strip out all punctuation by searching for any character that is not a letter and replacing it with a space. Using the re module is the most efficient and reliable way to handle these specific cleaning needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "020f5dcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Comparing Original Text with Cleaned Text:\n",
      "                                                Text  \\\n",
      "0  These singles sell for $2.50 - $3.36 at the st...   \n",
      "1  This Australian ginger is the best.  The ginge...   \n",
      "2  I used to be able to buy these at our local gr...   \n",
      "3  I decided to try Feline Pine because I was sic...   \n",
      "4  We love this seasoning.  We use it on Stake an...   \n",
      "\n",
      "                                           full_text  \n",
      "0  rip off price these singles sell for the store...  \n",
      "1  delicious ginger this australian ginger the be...  \n",
      "2  really delicious used able buy these our local...  \n",
      "3  one the worst cat litter have ever used decide...  \n",
      "4  greek spice don love this seasoning use stake ...  \n"
     ]
    }
   ],
   "source": [
    "#Define the text cleaning function\n",
    "#import regex for text processing\n",
    "import re\n",
    "\n",
    "#Define a function to clean text\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Our goal with this function is to take a text string and clean it up for analysis.\n",
    "    Specifically, we want to:\n",
    "    1. Removes HTML tags\n",
    "    2. Converts text to lowercase\n",
    "    3. Removes punctuation\n",
    "    \"\"\"\n",
    "    \n",
    "    #Remove HTML tags using a regular expression\n",
    "    clean = re.sub('<.*?>', '', text)\n",
    "    \n",
    "    #Convert to lowercase\n",
    "    clean = clean.lower()\n",
    "    \n",
    "    #Remove punctuation. This regex keeps only letters and replaces everything else with a space.\n",
    "    clean = re.sub('[^a-zA-Z]', ' ', clean)\n",
    "    \n",
    "    #Remove extra whitespace. Possibly not necessary, but it helps to ensure clean text.\n",
    "    clean = ' '.join(clean.split())\n",
    "    \n",
    "    #Split text into a list of words\n",
    "    words = clean.split()\n",
    "    \n",
    "    #Remove short words (e.g., 's', 't', 'aa', 'bc')\n",
    "    #We keep words that are 3 or more characters long.\n",
    "    words = [word for word in words if len(word) > 2]\n",
    "    \n",
    "    # Join the words back into a single string\n",
    "    clean = ' '.join(words)\n",
    "    \n",
    "    \n",
    "    return clean\n",
    "\n",
    "#Apply the function to the 'Text' column\n",
    "#We create a new column 'cleaned_text' to store the results.\n",
    "df['cleaned_text'] = df['Text'].apply(clean_text)\n",
    "df['cleaned_summary'] = df['Summary'].apply(clean_text)\n",
    "\n",
    "# 2. Create a new column by combining the cleaned summary and text\n",
    "# Adding a space in between ensures the last word of the summary and the\n",
    "# first word of the text don't merge together.\n",
    "df['full_text'] = df['cleaned_summary'] + ' ' + df['cleaned_text']\n",
    "\n",
    "#Display the results to compare the original vs. cleaned text.\n",
    "print(\"\\nComparing Original Text with Cleaned Text:\")\n",
    "print(df[['Text', 'full_text']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cccec3e0",
   "metadata": {},
   "source": [
    "While displaying our data shows a small change for now, most notably that our summary is tacked on in front of the review, our text will be ready for when we use TF-IDF for when we use it. Before that however, we'll have to separate our features from our target, and perform the test-train split.\n",
    "\n",
    "## 4. Test-Train Split\n",
    "\n",
    "With our data clean, it's time to split our data into training and testing data. We want to split our data into training and testing so that there is a set of data our model can learn from, and a set of data to practice against. We can do this simply using the train_test_split module from Sklearn.\n",
    "\n",
    "Before using the function we'll split the target we engineered from the rest of our features as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "00024a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import the train_test_split function\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#Separate features and target variable\n",
    "X = df['cleaned_text']\n",
    "y = df['sentiment']\n",
    "\n",
    "#Split the data into training and testing sets\n",
    "#Stratify ensures that the proportion of positive and negative reviews is maintained in both sets.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=64, stratify=y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8268e984",
   "metadata": {},
   "source": [
    "## 5. Feature Extraction\n",
    "\n",
    "With our data cleaned and split, we can now proceed to the most critical step of our NLP pipeline: feature extraction. This is the process where we will finally convert our cleaned review text into a numerical format that a machine learning model can understand and learn from. For this project, as outlined in our goals, we will be using a powerful and standard technique called TF-IDF (Term Frequency-Inverse Document Frequency).\n",
    "\n",
    "TF-IDF is an intelligent way to represent text that goes beyond simply counting how many times a word appears. It works by calculating a score for each word in each review, a score that reflects how important that word is to that specific document. This is done by balancing two metrics: Term Frequency (TF), which is how often a word appears in a single review, and Inverse Document Frequency (IDF), which lowers the score for words that are very common across all reviews (like \"and\" or \"it\") and boosts the score for words that are rare and more descriptive. The result is a numerical feature for each word that effectively represents its importance.\n",
    "\n",
    "Our plan is to use the TfidfVectorizer from the Sklearn library to implement this. It's crucial that we perform this step correctly to avoid data leakage: we will fit the vectorizer only on our training data to learn the vocabulary and word importances, and then use that same fitted vectorizer to transform both our training and test sets into numerical matrices. By setting parameters like `max_features`, we can also control the size of our vocabulary to keep our model efficient and focused on only the most relevant terms. Another parameter we'll use is `stop_words`, which we mentioned before will deal with incredibly common words like \"the\".\n",
    "\n",
    "Let's start by importing the module from Sklearn. We'll set up an instance of the TF-IDF object, fit it on the training data, and use it to transfrom both training and testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93dea1db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing TfidfVectorizer...\n",
      "Vectorizer initialized.\n",
      "\n",
      "Fitting and transforming X_train...\n",
      "X_train transformed.\n",
      "Transforming X_test...\n",
      "X_test transformed.\n",
      "\n",
      "--- Data Transformation Analysis ---\n",
      "Original shape of X_train: (91376,)\n",
      "Original shape of X_test:  (22844,)\n",
      "\n",
      "Shape of X_train after TF-IDF: (91376, 10000)\n",
      "Shape of X_test after TF-IDF:  (22844, 10000)\n",
      "\n",
      "Number of learned features (vocabulary size): 10000\n",
      "A few example features: ['aafco', 'abandoned', 'abc', 'abdominal', 'ability', 'able', 'abroad', 'absence', 'absent', 'absolute']\n",
      "\n",
      "Example Transformation ---\n",
      "Original first review in X_test: 'someone gave these olives gift basket christmas last year they are amazing buying them for others this year'\n",
      "\n",
      "Transformed TF-IDF vector for that review (sparse format):\n",
      "<Compressed Sparse Row sparse matrix of dtype 'float64'\n",
      "\twith 8 stored elements and shape (1, 10000)>\n",
      "  Coords\tValues\n",
      "  (0, 254)\t0.30040135019160796\n",
      "  (0, 627)\t0.38705181677153544\n",
      "  (0, 1122)\t0.2336722662803621\n",
      "  (0, 1510)\t0.32606234996460853\n",
      "  (0, 3653)\t0.2650253056393905\n",
      "  (0, 3699)\t0.2913707944659013\n",
      "  (0, 6033)\t0.4182250099791549\n",
      "  (0, 9939)\t0.5186357338005764\n"
     ]
    }
   ],
   "source": [
    "#Import the TFIDFVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "#Initialize the TfidfVectorizer.\n",
    "#This is where we set all our rules for feature creation.\n",
    "tfidf_vectorizer = TfidfVectorizer(\n",
    "    stop_words='english',  #Use the built-in English stop word list\n",
    "    max_features=10000,    #Keep only the top 10,000 words\n",
    "    min_df=5,              #We'll keep all words that appear at least 5 times\n",
    "    max_df=0.8             #Ignore words that appear in more than 80% of reviews\n",
    ")\n",
    "\n",
    "#Fit the vectorizer and transform the training data.\n",
    "#This learns the vocabulary from X_train and converts it to a numerical matrix.\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "\n",
    "#Transform the test data.\n",
    "#This uses the vocabulary learned from X_train to transform X_test.\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
    "\n",
    "#Analyzing the Transformed Data\n",
    "#Show the original shape of the data (a 1D series of text)\n",
    "print(f\"Original shape of X_train: {X_train.shape}\")\n",
    "print(f\"Original shape of X_test:  {X_test.shape}\")\n",
    "\n",
    "#Show the new shape of the data after TF-IDF\n",
    "#It is now a 2D matrix where columns are words from the vocabulary.\n",
    "print(f\"\\nShape of X_train after TF-IDF: {X_train_tfidf.shape}\")\n",
    "print(f\"Shape of X_test after TF-IDF:  {X_test_tfidf.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91340f92",
   "metadata": {},
   "source": [
    "As we can see, using TF-IDF ended up creating 10,000 features for the words in the review. The idea is that each of these words are given a score based on how frequently they show up in a single review and how frequently they show up in reviews in general. These scores help our model determine the sentiment of certain words, and review overall. \n",
    "\n",
    "## 6. Building and Training the Models\n",
    "Now that our text data has been cleaned and transformed into a numerical format using TF-IDF, we are ready to proceed with the model building and training phase. For this project, we will implement two different classification models: Logistic Regression and Random Forest. Using both allows us to establish a strong, interpretable baseline and then see if a more complex model can improve upon it. Logistic Regression is an excellent starting point as it is very fast to train and highly interpretable, which will allow us to easily explore the word-level features that are most predictive of sentiment. We will then train a Random Forest model, a more powerful ensemble method, to determine if its complexity leads to an increase in predictive performance.\n",
    "\n",
    "We'll start by importing both models from Sklearn. We'll then intialize them, and fit them on the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceacd72b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Initializing classifiers...\n",
      "\n",
      "Fitting Logistic Regression model...\n",
      "Fitting Random Forest model...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: #000;\n",
       "  --sklearn-color-text-muted: #666;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-2 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-2 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-2 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: flex;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "  align-items: start;\n",
       "  justify-content: space-between;\n",
       "  gap: 0.5em;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 label.sk-toggleable__label .caption {\n",
       "  font-size: 0.6rem;\n",
       "  font-weight: lighter;\n",
       "  color: var(--sklearn-color-text-muted);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"â–¸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"â–¾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-2 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-2 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-2 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-2 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-2 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 0.5em;\n",
       "  text-align: center;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-2 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-2 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestClassifier(random_state=64)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>RandomForestClassifier</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.6/modules/generated/sklearn.ensemble.RandomForestClassifier.html\">?<span>Documentation for RandomForestClassifier</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></div></label><div class=\"sk-toggleable__content fitted\"><pre>RandomForestClassifier(random_state=64)</pre></div> </div></div></div></div>"
      ],
      "text/plain": [
       "RandomForestClassifier(random_state=64)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Import logistic regression and random forest classifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "#Initialize the classifiers\n",
    "logistic_model = LogisticRegression(max_iter=1000, random_state=64)\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=64)\n",
    "\n",
    "#Fit the logistic regression model\n",
    "logistic_model.fit(X_train_tfidf, y_train)\n",
    "\n",
    "#Fit the random forest model\n",
    "rf_model.fit(X_train_tfidf, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab31f717",
   "metadata": {},
   "source": [
    "You might notice that the Random Forest took quite a while to be fit. While there are times where the dataset is seemingly larger with the model fitting faster, having 10,000 features to go through made the model take quite a bit of time. That's alright however, and the further advanced our projects get, the longer such actions might take as well.\n",
    "\n",
    "## 7. Evaulating our Models.\n",
    "With both models trained on our cleaned data, it's time to test our models to so how well they perform. We'll import several metrics of success from Sklearn. Fortunately these metrics work with both models, so we can go straight ahead and see how they perform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3b306c0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy (Logistic Regression): 0.8831640693398705\n",
      "Validation Accuracy (Random Forest): 0.8554543862721065\n",
      "Confusion Matrix (Logistic Regression):\n",
      " [[10114  1308]\n",
      " [ 1361 10061]]\n",
      "Confusion Matrix (Random Forest):\n",
      " [[9875 1547]\n",
      " [1755 9667]]\n",
      "Classification Report (Logistic Regression):\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.89      0.88     11422\n",
      "           1       0.88      0.88      0.88     11422\n",
      "\n",
      "    accuracy                           0.88     22844\n",
      "   macro avg       0.88      0.88      0.88     22844\n",
      "weighted avg       0.88      0.88      0.88     22844\n",
      "\n",
      "Classification Report (Random Forest):\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.86      0.86     11422\n",
      "           1       0.86      0.85      0.85     11422\n",
      "\n",
      "    accuracy                           0.86     22844\n",
      "   macro avg       0.86      0.86      0.86     22844\n",
      "weighted avg       0.86      0.86      0.86     22844\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#import metrics of success from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "#Make predictions on the validation set, and store it in a variable. \n",
    "y_pred1 = logistic_model.predict(X_test_tfidf)\n",
    "y_pred2 = rf_model.predict(X_test_tfidf)\n",
    "\n",
    "#Check the accuracy of our predictions\n",
    "print(\"Validation Accuracy (Logistic Regression):\", accuracy_score(y_test, y_pred1))\n",
    "print(\"Validation Accuracy (Random Forest):\", accuracy_score(y_test, y_pred2))\n",
    "\n",
    "#Display the confusion matrix\n",
    "print(\"Confusion Matrix (Logistic Regression):\\n\", confusion_matrix(y_test, y_pred1))\n",
    "print(\"Confusion Matrix (Random Forest):\\n\", confusion_matrix(y_test, y_pred2))\n",
    "\n",
    "#Print the classification report for more details\n",
    "print(\"Classification Report (Logistic Regression):\\n\", classification_report(y_test, y_pred1))\n",
    "print(\"Classification Report (Random Forest):\\n\", classification_report(y_test, y_pred2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f56925a",
   "metadata": {},
   "source": [
    "### Analysis\n",
    "\n",
    "#### Confusion Matrix\n",
    "The confusion matrices for both models show how well each approach distinguishes between negative reviews (class 0) and positive reviews (class 1). For logistic regression, the model correctly predicted 10114 negative reviews and 10061 positive reviews, but also misclassified 1308 negative reviews as positive, and 1361 positive reviews as negative. The random forest model follows a similar pattern, with 9875 correct predictions for negative reviews and 9667 for positive, while making more errors overall: 1547 false positives and 1755 missed positive reviews. Both models show strong performance overall, though logistic regression appears more balanced and slightly more accurate in distinguishing sentiment polarity.\n",
    "\n",
    "#### Classification Report\n",
    "Precision, recall, and F1-score offer additional insight into the modelsâ€™ respective strengths:\n",
    "\n",
    "- Precision: For negative reviews (class 0), both models are strong, with precision at 0.88 for logistic regression and 0.85 for random forest, indicating that most negative predictions are indeed correct. The same holds true for positive reviews (class 1), with precision at 0.88 for logistic regression and 0.86 for random forest. This reflects consistent confidence across both classes.\n",
    "\n",
    "- Recall: Logistic regression achieves nearly identical recall for both classes, at 0.89 for negatives and 0.88 for positives, suggesting the model is adept at correctly identifying both types of sentiment. Random forest lags slightly behind, with 0.86 recall for negative reviews and 0.85 for positive reviews, showing a minor dip in its ability to recognize the true class labels.\n",
    "\n",
    "- F1-score: F1-scores are evenly matched between the two classes in both models. Logistic regression yields an F1-score of 0.88 across the board, showing balanced precision and recall. Random forest, while slightly lower at 0.86, still maintains respectable performance. The consistency across precision, recall, and F1 in logistic regression indicates stronger overall robustness.\n",
    "\n",
    "The macro and weighted averages mirror these findings, hovering around 0.88 for logistic regression and 0.86 for random forest. This suggests that class imbalance is not an issue (thanks to our prior balancing), and that performance is uniform across both sentiment categories.\n",
    "\n",
    "Overall Analysis\n",
    "Validation accuracy for logistic regression stands at about 0.88, slightly outperforming random forest at 0.86. While both models are strong performers, logistic regression shows better balance, fewer errors, and slightly stronger classification power. The close alignment of its precision, recall, and F1-score for both classes makes it particularly reliable. In contrast, the random forest model, while still highly capable, makes more mistakes and shows slightly lower consistency.\n",
    "\n",
    "Given the size of the dataset and the nature of the task, logistic regression emerges as a well-suited baseline model. Its solid performance, combined with efficiency and interpretability, makes it a great choice for binary sentiment analysis. In a practical setting, one could consider further tuning the models, adding feature engineering, or exploring more advanced architectures like neural nets if higher performance is needed. But for many use cases, this level of accuracy and stability is already actionable.\n",
    "\n",
    "Comparing both models highlights not only their strengths and limitations, but also underscores the value of using multiple metrics beyond accuracy. Whether youâ€™re looking to understand product sentiment, streamline customer feedback, or just explore the world of natural language processing, these results are a strong foundation to build on.\n",
    "\n",
    "\n",
    "## 8. Exploring Word Level Features \n",
    "\n",
    "Now that we have confirmed that our models are performing well, we can move beyond simply measuring their predictive accuracy to understanding how they arrive at their conclusions. The next logical step is to explore the word-level features that the model identified as the most powerful predictors of sentiment. The primary reason for this analysis is interpretability; we want to validate that the model is learning logical patterns from the text and gain direct insight into the specific language that drives positive and negative reviews.\n",
    "\n",
    "To accomplish this, we will inspect our trained Logistic Regression model, as it is highly transparent, especially compared to our Random Forest. This model assigns a numerical weight, or coefficient, to every word in the vocabulary created by our TF-IDF vectorizer. By extracting and ranking these coefficients, we can create a definitive list of the words with the strongest positive weights (the top predictors of a positive review) and those with the strongest negative weights (the top predictors of a negative review). This process effectively allows us to translate the model's internal logic into a clear and actionable analysis of customer sentiment.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10f59b69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Top 20 Most Powerful Predictors of a NEGATIVE Review ---\n",
      "              word    weight\n",
      "0            worst -9.016542\n",
      "1    disappointing -8.631358\n",
      "2            awful -7.999173\n",
      "3         terrible -7.943300\n",
      "4     disappointed -7.678569\n",
      "5         horrible -7.526429\n",
      "6   disappointment -7.456513\n",
      "7    unfortunately -6.595148\n",
      "8            bland -6.380178\n",
      "9           return -5.968935\n",
      "10           threw -5.859926\n",
      "11           waste -5.725940\n",
      "12           stale -5.674093\n",
      "13          hoping -5.634528\n",
      "14            weak -5.589291\n",
      "15      disgusting -5.437771\n",
      "16       tasteless -5.319889\n",
      "17           sorry -5.193882\n",
      "18         thought -5.042910\n",
      "19           worse -5.022502\n",
      "\n",
      "============================================================\n",
      "\n",
      "--- Top 20 Most Powerful Predictors of a POSITIVE Review ---\n",
      "          word     weight\n",
      "0        great  10.407474\n",
      "1    delicious   9.506926\n",
      "2         best   9.283243\n",
      "3       highly   7.976964\n",
      "4      perfect   7.831176\n",
      "5    excellent   6.791275\n",
      "6         love   6.649609\n",
      "7    wonderful   6.649027\n",
      "8        loves   6.561121\n",
      "9         good   5.986042\n",
      "10     amazing   5.945671\n",
      "11    favorite   5.523752\n",
      "12     pleased   5.474018\n",
      "13      hooked   5.386439\n",
      "14       yummy   5.251717\n",
      "15        nice   5.241427\n",
      "16     awesome   5.234656\n",
      "17        beat   5.220004\n",
      "18       thank   5.181713\n",
      "19  pleasantly   5.036770\n"
     ]
    }
   ],
   "source": [
    "#Get the feature names (words) from the TfidfVectorizer\n",
    "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "\n",
    "#Get the coefficients from the trained Logistic Regression model\n",
    "#The [0] is because we have a binary classification problem\n",
    "coefficients = logistic_model.coef_[0]\n",
    "\n",
    "#Create a DataFrame to hold the words and their corresponding weights\n",
    "feature_weights = pd.DataFrame({\n",
    "    'word': feature_names,\n",
    "    'weight': coefficients\n",
    "})\n",
    "\n",
    "#Sort the DataFrame by the weights to find the most influential words\n",
    "#Ascending sort will show the most negative words first\n",
    "most_negative_words = feature_weights.sort_values(by='weight', ascending=True)\n",
    "\n",
    "#Descending sort will show the most positive words first\n",
    "most_positive_words = feature_weights.sort_values(by='weight', ascending=False)\n",
    "\n",
    "#Display the results\n",
    "print(\"Top 20 Most Powerful Predictors of a NEGATIVE Review:\")\n",
    "\n",
    "#reset_index(drop=True) makes the output cleaner\n",
    "print(most_negative_words.head(20).reset_index(drop=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad038a6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Top 20 Most Powerful Predictors of a POSITIVE Review ---\n",
      "          word     weight\n",
      "0        great  10.407474\n",
      "1    delicious   9.506926\n",
      "2         best   9.283243\n",
      "3       highly   7.976964\n",
      "4      perfect   7.831176\n",
      "5    excellent   6.791275\n",
      "6         love   6.649609\n",
      "7    wonderful   6.649027\n",
      "8        loves   6.561121\n",
      "9         good   5.986042\n",
      "10     amazing   5.945671\n",
      "11    favorite   5.523752\n",
      "12     pleased   5.474018\n",
      "13      hooked   5.386439\n",
      "14       yummy   5.251717\n",
      "15        nice   5.241427\n",
      "16     awesome   5.234656\n",
      "17        beat   5.220004\n",
      "18       thank   5.181713\n",
      "19  pleasantly   5.036770\n"
     ]
    }
   ],
   "source": [
    "#And our positive words. \n",
    "#This is in a separate cell since the output was truncated in the previous cell.\n",
    "print(\"Top 20 Most Powerful Predictors of a POSITIVE Review:\")\n",
    "print(most_positive_words.head(20).reset_index(drop=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "431cc555",
   "metadata": {},
   "source": [
    "### Analysis of Word-Level Features\n",
    "An inspection of the model's learned coefficients provides powerful validation for our entire pipeline. The lists of the most influential words for both positive and negative sentiment align perfectly with human intuition, which gives us high confidence that the model has learned meaningful patterns from the text data rather than relying on noise.\n",
    "\n",
    "For negative reviews, the model identified words like worst, disappointing, awful, and terrible as the strongest predictors. The inclusion of more specific, product-related terms such as bland, stale, and tasteless, as well as action-oriented words like return and threw, further demonstrates the model's ability to grasp the nuances of customer dissatisfaction.\n",
    "\n",
    "Conversely, the top positive words are dominated by superlatives and enthusiastic descriptors. Words like great, delicious, best, perfect, and excellent were assigned the highest positive weights, clearly signaling strong customer satisfaction. The presence of words like love, favorite, and pleasantly reinforces the model's successful identification of positive emotional language. The fact that the model learned these word associations on its own is a testament to the effectiveness of the TF-IDF feature extraction and the robustness of the classifier.\n",
    "\n",
    "### Real-World Applications\n",
    "This analysis is far more than just an academic exercise, as it provides actionable business intelligence. A company could use these insights in several key ways:\n",
    "\n",
    "- Automated Customer Support: The sentiment model could be used to automatically tag and route incoming customer feedback. A review containing words with strong negative weights could be immediately flagged and sent to a support team for priority follow-up, enabling proactive customer service.\n",
    "\n",
    "- Product Development Insights: By analyzing the most common negative keywords (e.g., bland, stale), a company can identify specific, recurring issues with their products and direct their quality assurance or product development teams to address them.\n",
    "\n",
    "- Marketing and Voice of Customer Analysis: The top positive keywords provide a clear picture of what customers value most. A marketing team could leverage this language, using words like delicious or perfect in their campaigns, knowing that these terms resonate strongly with their satisfied customers.\n",
    "\n",
    "### Conclusion\n",
    "Congratulations on reaching the end of this project! You have successfully navigated a complete Natural Language Processing pipeline, from handling a large, messy, real-world dataset to performing sophisticated text cleaning and feature extraction. You've built and evaluated multiple models and, most importantly, have interpreted their internal logic to extract meaningful, actionable insights. This project demonstrates a deep understanding of both the technical and analytical skills required for sentiment analysis. Job well done!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
